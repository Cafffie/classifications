# -*- coding: utf-8 -*-
"""bank_customer_churn_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e0MxUkFBafTyt7IVa2pdkMqashZSoOyz
"""

import pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import pickle
import warnings
warnings.filterwarnings("ignore")

df= pd.read_csv("sailchurn.txt", sep= '\t')
data= df[:]
df.head()

df.shape

df.sample(n=5, replace=False)

df.info()

round(df.describe(), 2)

round(df.describe(include='object'), 2).T

"""## **Data Cleaning and Preprocessing**"""

df.columns

df.isnull().sum()

# Print the unique values in each column
for column in df:
  print(column, ':', df[column].unique())

df= df.dropna(subset='RISK_RATING')

df.RISK_RATING.unique()

df.isnull().values.any()

len(df.Acct_ID.unique())

"""The total no of column is 500,000, this shows that there are duplicate values"""

duplicates= df.duplicated(subset= 'Acct_ID')
len(df[duplicates])

"""There are 10,925 duplicated rows"""

df[duplicates].sample(5)

df= df.drop_duplicates(subset='Acct_ID')
len(df)

df.columns

labels= {
    ' AVE BAL ': "AVERAGE_BALANCE",
    ' LAST_12_MONTHS_DEBIT_VALUE ': 'LAST_12_MONTHS_DEBIT_VALUE',
    ' LAST_12_MONTHS_CREDIT_VALUE ': 'LAST_12_MONTHS_CREDIT_VALUE'
}
df= df.rename(columns= labels)
df.sample(5)

df= df.drop('Acct_ID', axis="columns")

df['AVERAGE_BALANCE'].values

df['LAST_12_MONTHS_DEBIT_VALUE'].values

df['LAST_12_MONTHS_CREDIT_VALUE'].values

df['LAST_12_MONTHS_DEBIT_VOLUME'].values

df['LAST_12_MONTHS_CREDIT_VOLUME'].values

# Remove white space from values these columns
columns_with_withspace= ['AVERAGE_BALANCE','LAST_12_MONTHS_DEBIT_VALUE','LAST_12_MONTHS_CREDIT_VALUE']
df[columns_with_withspace]= df[columns_with_withspace].apply(lambda x: x.str.strip())

df['LAST_12_MONTHS_CREDIT_VALUE'].values

df['LAST_12_MONTHS_DEBIT_VALUE'].values

df['AVERAGE_BALANCE'].values

# number of '-' in AVERAGE_BALANCE
df[df['AVERAGE_BALANCE']== '-'][:5]

len(df[df.AVERAGE_BALANCE =='-'])

len(df[df.LAST_12_MONTHS_CREDIT_VALUE== '-'])

len(df[df.LAST_12_MONTHS_DEBIT_VALUE== '-'])

df['AVERAGE_BALANCE'] = df['AVERAGE_BALANCE'].astype(str).str.replace('(', '').replace(')', '').replace(',', '')

df['AVERAGE_BALANCE']= df['AVERAGE_BALANCE'].astype(str).str.replace("(", "")
df['AVERAGE_BALANCE']= df['AVERAGE_BALANCE'].astype(str).str.replace(")", "")
df['AVERAGE_BALANCE']= df['AVERAGE_BALANCE'].astype(str).str.replace(",", "")

df['LAST_12_MONTHS_DEBIT_VALUE']= df['LAST_12_MONTHS_DEBIT_VALUE'].astype(str).str.replace(',', '')
df['LAST_12_MONTHS_CREDIT_VALUE']= df['LAST_12_MONTHS_CREDIT_VALUE'].astype(str).str.replace(',', '')

df['AVERAGE_BALANCE'].values

df['LAST_12_MONTHS_DEBIT_VALUE'].values

df['LAST_12_MONTHS_CREDIT_VALUE'].values

df['AVERAGE_BALANCE']= df['AVERAGE_BALANCE'].replace('-', np.nan).astype(float)
average_balance_mean= round(df['AVERAGE_BALANCE'].mean(axis=0), 2)
average_balance_mean

df['LAST_12_MONTHS_DEBIT_VALUE']= df['LAST_12_MONTHS_DEBIT_VALUE'].replace('-', np.nan).astype(float)
debit_value_mean= round(df['LAST_12_MONTHS_DEBIT_VALUE'].mean(axis=0), 2)
debit_value_mean

df['LAST_12_MONTHS_CREDIT_VALUE']= df['LAST_12_MONTHS_CREDIT_VALUE'].replace('-', np.nan).astype(float)
credit_value_mean= round(df['LAST_12_MONTHS_CREDIT_VALUE'].mean(axis=0), 2)
credit_value_mean

df['AVERAGE_BALANCE'].replace(np.nan, average_balance_mean, inplace=True)
df['LAST_12_MONTHS_DEBIT_VALUE'].replace(np.nan, debit_value_mean, inplace=True)
df['LAST_12_MONTHS_CREDIT_VALUE'].replace(np.nan, credit_value_mean, inplace=True)

df['AVERAGE_BALANCE'].values

df['LAST_12_MONTHS_DEBIT_VALUE'].values

df['LAST_12_MONTHS_CREDIT_VALUE'].values

df_uniques= df.nunique()
df_uniques

df_uniques[(df_uniques > 2) & (df_uniques < 15)]

binary_variable= list(df_uniques[df_uniques == 2].index)
binary_variable

list(df_uniques[df_uniques < 15].index)

categorical_variables= list(df_uniques[df_uniques < 15].index)
categorical_variables

# show a list of columns in the categorical variables and a list of its contents
[[i, list(df[i].unique())] for i in categorical_variables]

ordinal_variable= ['YEARS_WITH_BANK', 'RISK_RATING']

continuous_variables= list(set(df.columns) - set(categorical_variables))
continuous_variables

df[continuous_variables].info()

df[continuous_variables]= df[continuous_variables].astype(float)

"""## **Univariate Analysis**"""

ax= df.CHURN.value_counts().plot.bar(color=['green', 'blue'])
ax.set(xlabel='Churn', ylabel='No of customers')
plt.show()

plt.figure(figsize=(15, 15))
for i, column in enumerate(categorical_variables, 1):
  plt.subplot(5, 3, i)
  sns.countplot(y=df[column], palette= 'Set3')
  plt.title(column)
  plt.tight_layout()
plt.show()

df[continuous_variables].hist(color= 'blue', figsize=(8, 6))
plt.show()

skew_limit= 0.75
skew_values= df[continuous_variables].skew()
skew_values

skew_columns= (skew_values
               .sort_values(ascending=False)
               .to_frame()
               .rename(columns={0: 'skew'})
               .query('abs(skew) > {}'.format(skew_limit)))
skew_columns

col= 'LAST_12_MONTHS_DEBIT_VALUE'

fig, (ax_before, ax_after)= plt.subplots(1, 2, figsize=(10, 5))
df[col].hist(ax= ax_before)
df[col].apply(np.log1p).hist(ax=ax_after)

ax_before.set(title= 'Before np.log1p', ylabel= 'frequency', xlabel='value')
ax_after.set(title= 'After np.log1p', ylabel= 'frequency', xlabel='value')
fig.suptitle('Col "{}"'.format(col))
plt.show()

for column in skew_columns.index.values:
  #if column== 'LAST_12_MONTHS_DEBIT_VALUE':
    #continue
  df[column]= df[column].apply(np.log1p)

df[continuous_variables].hist(color='blue', figsize=(8, 6))
plt.tight_layout()
plt.show()

sns.pairplot(df[continuous_variables])

categorical_variables= list(set(categorical_variables)- set(['CHURN']) - set(ordinal_variable))
categorical_variables

df2= pd.get_dummies(df, columns= categorical_variables, drop_first=True, dtype=int)
df2.sample(5)

le= LabelEncoder()

for column in ordinal_variable:
  df2[column]= le.fit_transform(df2[column])

df2[ordinal_variable].astype('category').describe()

df2[:5]

"""## **Standardization**"""

sc= StandardScaler()
df2[continuous_variables]= sc.fit_transform(df2[continuous_variables])
df2[continuous_variables][:5]

"""## **Train and test set splitting**"""

# Set up X and y variables
y, X= df2['CHURN'], df2.drop(columns='CHURN')

y.shape, X.shape

# Split train and test set

X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=234)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

y_train.value_counts()

y_test.value_counts()

"""## **Model Building**

## **Gradient Boosting Classifier**
"""

error_list= list()
tree_list= [15, 25, 50, 100, 200, 400]

for n_trees in tree_list:
  GBC= GradientBoostingClassifier(n_estimators=n_trees, random_state=234)

  # Fit the model
  print(f"Fitting model with {n_trees} trees")
  GBC.fit(X_train.values, y_train.values)
  y_pred= GBC.predict(X_test)

  # Get the error
  error= 1 - accuracy_score(y_pred, y_test)

  # Store the error
  error_list.append(pd.Series({"n_trees": n_trees, "error": error}))

error_df= pd.concat(error_list, axis=1).T.set_index('n_trees')
error_df

import pickle

# Save the model
pickle.dump(GBC, open('gbc.p', 'wb'))

# Load the model
GBC= pickle.load(open('gbc.p', 'rb'))

"""### **Plot the result**"""

#sns.set_context("talk")
#sns.set_style('white')

ax= error_df.plot(marker='o', figsize=(6, 6), linewidth=5)
ax.set(xlabel= 'No of tree', ylabel='Error')
ax.set_xlim(0, max(error_df.index)*1.1)
plt.show()

print(classification_report(y_test, y_pred))

# Function to generate test accuracy and training accuracy
def get_accuracy(X_train, X_test, y_train, y_test, model):
  return{'Test Accuracy':accuracy_score(y_test, model.predict(X_test)),
         'Training Accuracy': accuracy_score(y_train, model.predict(X_train))}

# Check for overfitting
get_accuracy(X_train, X_test, y_train, y_test, GBC)

sns.set_context('talk')
cm= confusion_matrix(y_test, y_pred)
ax= sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')

"""## **XGBoost Classifier**"""

xgb= XGBClassifier()
xgb.fit(X_train, y_train)
xgb_pred= xgb.predict(X_test)

print(classification_report(xgb_pred, y_test))

get_accuracy(X_train, X_test, y_train, y_test, xgb)

tree_list = [50, 100, 200, 400]

best_score = 0
best_model = None
best_n_tree = None

for n_tree in tree_list:
    model = XGBClassifier(objective='binary:logistic',
                          n_estimators=n_tree,
                          eval_metric='mlogloss')

    print(f"Fitting model with {n_tree} trees")
    model.fit(X_train.values, y_train.values)
    y_pred = model.predict(X_test)

    # Evaluate the model
    score = accuracy_score(y_test, y_pred)
    print(f"Accuracy for {n_tree} trees: {score:.4f}")

    # Check if this model is the best
    if score > best_score:
        best_score = score
        best_model = model
        best_n_tree = n_tree

# Print the best score and the best number of trees
print(f"\nBest accuracy score: {best_score:.4f} with {best_n_tree} trees")

# Save the best model to a file
with open('best_xgb_model.p', 'wb') as file:
    pickle.dump(best_model, file)

print("Best model saved as 'best_xgb_model.p'")

model= pickle.load(open('best_xgb_model.p', 'rb'))

get_accuracy(X_train, X_test, y_train, y_test, model)

y_pred= model.predict(X_test)
print(classification_report(y_pred, y_test))

sns.set_context("talk")
cm= confusion_matrix(y_pred, y_test)
ax= sns.heatmap(cm, annot=True, fmt= 'd', cmap= 'Blues')

"""### **Tuning XGBoost Classifier with gridsearch**"""

XGB= XGBClassifier(objective='binary:logistic', eval_metric='mlogloss')

param_grid= {'n_estimators': [50, 100, 200, 400],
             'learning_rate': [0.1, 0.01, 0.001]}

search= GridSearchCV(estimator= XGB,
                     param_grid= param_grid,
                     scoring= "accuracy")
search.fit(X_train, y_train)

print(get_accuracy(X_train, X_test, y_train, y_test, search))

search.best_score_

search.best_params_

best_xgb= search.best_estimator_

# Save the model
pickle.dump(best_xgb, (open('xgb_Bchurn.p', 'wb')))

# Load the model
model2= pickle.load(open('xgb_Bchurn.p', 'rb'))

gs_pred= model2.predict(X_test)
print(classification_report(gs_pred, y_test))

sns.set_context("talk")
cm= confusion_matrix(gs_pred, y_test)
ax= sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')